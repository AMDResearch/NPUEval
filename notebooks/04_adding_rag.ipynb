{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dd398ad-9d7d-45d4-8272-b425a9538a92",
   "metadata": {},
   "source": [
    "# Retrieval augmented generation (RAG) with vectorized kernels\n",
    "\n",
    "This notebook demonstrates how to use LlamaIndex, a popular RAG framework to index our reference kernel database and use it to improve our LLM code generation capabilities.\n",
    "\n",
    "## Goals\n",
    "\n",
    "* Index a vector database of vectorized kernels\n",
    "* Create a basic retriever\n",
    "* Generate new kernels solutions with retrieved kernels in-context\n",
    "* Compared RAG results with previous implementation\n",
    "\n",
    "## References\n",
    "* https://docs.llamaindex.ai/en/stable/getting_started/starter_example/\n",
    "* https://github.com/AMDResearch/Riallto/tree/main/npu/lib/kernels/cpp\n",
    "* https://github.com/Xilinx/mlir-aie/tree/main/aie_kernels/aie2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11e7b908-14c8-4cd5-9035-afaef0e88976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LlamaIndex essentials\n",
    "from llama_index.core import (\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caefb2b-bdb4-4076-9c29-3b64bee3b3f9",
   "metadata": {},
   "source": [
    "**Important:** Set your OpenAI API key as an environment variable before running this notebook.  \n",
    "Example:\n",
    "```python\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-...your-key-here...'\n",
    "```\n",
    "\n",
    "## Indexing the kernel database\n",
    "\n",
    "We've currated a handful of open-source kernels and stored them in `rag/kernels` of this repo - the contained C++ sources will be used to create our vector index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51f42a0b-e7bd-422c-bbef-2e8fa56a8a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Where the vector store will live\n",
    "PERSIST_DIR = \"./rag/vector_database\"\n",
    "\n",
    "# If the database already exists let's not re-index everything\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    print(\"Indexing...\")\n",
    "    documents = SimpleDirectoryReader(\"../rag/kernels\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae667ad-3469-4a1d-88de-e3e90a2247a4",
   "metadata": {},
   "source": [
    "## Retrieving Similar Kernels\n",
    "\n",
    "Let's test this out by retrieving some nodes for a given prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62882e26-93de-49f5-ac72-d68877a88407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu.cc\n",
      "0.8285732809964754\n",
      "conv2dk1_i8.cc\n",
      "0.7933807263913192\n"
     ]
    }
   ],
   "source": [
    "retriever = index.as_retriever(similarity_top_k=2)\n",
    "nodes = retriever.retrieve(\"A ReLU kernel\")\n",
    "\n",
    "for node in nodes:\n",
    "    print(node.metadata['file_name'])\n",
    "    print(node.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef46d0c-5f50-4924-a81b-6fb502e33162",
   "metadata": {},
   "source": [
    "Great! The top result is a ReLU kernel which is the ideal choice for this prompt.\n",
    "\n",
    "## Using a Real Kernel Prompt\n",
    "\n",
    "Now, let's use a real NPUEval prompt and see how our retriever does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cd91e02-4394-4b21-9c06-4cabfa5b105d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plus1.cpp\n",
      "0.87517622154528\n",
      "reduce_add.cc\n",
      "0.8658736014149861\n"
     ]
    }
   ],
   "source": [
    "from npueval import dataset\n",
    "sample = dataset.get_by_name(\"add_offset_int8\")\n",
    "\n",
    "nodes = retriever.retrieve(sample['prompt'])\n",
    "\n",
    "for node in nodes:\n",
    "    print(node.metadata['file_name'])\n",
    "    print(node.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9811f7ab-2daa-4747-a8d1-a749aab5fbbc",
   "metadata": {},
   "source": [
    "plus1 is actually a good choice for an add_offset kernel because functionally they are very similar. However add_offset needs to add a runtime parameter for the offset instead of having a hardcoded constant that gets added to the input vector.\n",
    "\n",
    "## Add context to the prompt\n",
    "\n",
    "Now we can craft a combined prompt with the additional context of the retrieved kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be1463f4-5c5c-45aa-9c4f-72151c7a0c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/*\n",
      "This AIE kernel adds a scalar int8 offset to every element of the input int8_t vector (length 256), and writes the result to the output buffer.\n",
      ">>> add_offset_int8([72, -53, 17, 92, -33, 95, 3, -91], -11)\n",
      "[61, -64, 6, 81, -44, 84, -8, -102]\n",
      "This kernel should be optimized for the following input/output buffer shapes and parameters:\n",
      "in_buffer size: 256\n",
      "out_buffer size: 256\n",
      "offset: -11\n",
      "*/\n",
      "#include <aie_api/aie.hpp>\n",
      "#include \"aie_kernel_utils.h\"\n",
      "\n",
      "void add_offset_int8(int8_t *in_buffer, int8_t *out_buffer, int8_t offset) {\n",
      "    // Implementation goes here\n",
      "}\n",
      "\n",
      "Reference vectorized code:\n",
      "// Copyright 2023 Advanced Micro Devices, Inc.\n",
      "// SPDX-License-Identifier: MIT\n",
      "\n",
      "#include <stdint.h>\n",
      "#include <stdio.h>\n",
      "#include <stdlib.h>\n",
      "#include <aie_api/aie.hpp>\n",
      "\n",
      "\n",
      "void plusone_aie(uint8_t *in_buffer, uint8_t* out_buffer, uint32_t nbytes) {\n",
      "    ::aie::vector<uint8_t, 32> buffer;\n",
      "    ::aie::vector<uint8_t, 32> inverted_buffer;\n",
      "    uint16_t loop_count = (nbytes) >> 5;\n",
      "    for(int j=0; j<loop_count; j++) {\n",
      "        buffer = ::aie::load_v<32>(in_buffer);\n",
      "        inverted_buffer = ::aie::add((uint8_t)1, buffer);\n",
      "        in_buffer += 32;\n",
      "        ::aie::store_v((uint8_t*)out_buffer, inverted_buffer);\n",
      "        out_buffer += 32;\n",
      "    }\n",
      "}\n",
      "\n",
      "extern \"C\" {\n",
      "\n",
      "void plusone(uint8_t *in_buffer, uint8_t* out_buffer, uint32_t nbytes) {\n",
      "    plusone_aie(in_buffer, out_buffer, nbytes);\n",
      "}\n",
      "\n",
      "}//===- reduce_add.cc --------------------------------------------*- C++ -*-===//\n",
      "//\n",
      "// This file is licensed under the Apache License v2.0 with LLVM Exceptions.\n",
      "// See https://llvm.org/LICENSE.txt for license information.\n",
      "// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n",
      "//\n",
      "// Copyright (C) 2023-2025, Advanced Micro Devices, Inc.\n",
      "//\n",
      "//===----------------------------------------------------------------------===//\n",
      "\n",
      "#include <stdint.h>\n",
      "#include <stdio.h>\n",
      "#include <stdlib.h>\n",
      "#include <type_traits>\n",
      "\n",
      "#include \"aie_kernel_utils.h\"\n",
      "#include <aie_api/aie.hpp>\n",
      "\n",
      "static void _reduce_add_vector(int32_t *restrict in, int32_t *res\n"
     ]
    }
   ],
   "source": [
    "context_string = \"Reference vectorized code:\\n\"\n",
    "for node in nodes:\n",
    "    context_string += node.node.text\n",
    "\n",
    "prompt_with_context = sample['prompt'] + \"\\n\" + context_string\n",
    "\n",
    "print(prompt_with_context[:2000]) # truncated output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1a5134-b9e6-4902-91fa-7cb8e6a356b4",
   "metadata": {},
   "source": [
    "## Generate new kernel code\n",
    "\n",
    "We'll generate a kernel with and without RAG and then compare how they did.\n",
    "\n",
    "### Without RAG\n",
    "\n",
    "As we've seen before GPT-4o-mini is very good at generating simple C++ code that will pass the functional tests. However this code is wholly unoptimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfc2ba62-b16f-4598-9ad3-5aebc1ec7d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code after 0 re-compilations:\n",
      "```cpp\n",
      "#include <aie_api/aie.hpp>\n",
      "#include \"aie_kernel_utils.h\"\n",
      "\n",
      "void add_offset_int8(int8_t *in_buffer, int8_t *out_buffer, int8_t offset) {\n",
      "    constexpr int size = 256;\n",
      "    // Load input data and add offset\n",
      "    for (int i = 0; i < size; i++) {\n",
      "        out_buffer[i] = in_buffer[i] + offset;\n",
      "    }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from npueval.aiecoder import AIECoder\n",
    "coder = AIECoder(model='gpt-4o-mini', temperature=0.4, attempts=5)\n",
    "response = coder(sample['prompt'])\n",
    "\n",
    "print(f\"Code after {response['attempt']} re-compilations:\")\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15611876-6750-4e02-a5b3-e8a3c05a21c2",
   "metadata": {},
   "source": [
    "### With RAG\n",
    "\n",
    "Now we'll reset our coder and generate again with a prompt containing an example of the plus1 kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "104cd87d-9020-49a1-b8d6-7cc063fb099e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code after 3 re-compilations:\n",
      "```cpp\n",
      "#include <aie_api/aie.hpp>\n",
      "\n",
      "void add_offset_int8(int8_t *in_buffer, int8_t *out_buffer, int8_t offset) {\n",
      "    ::aie::vector<int8_t, 64> buffer; // Use a vector of size 64 for loading\n",
      "    ::aie::vector<int8_t, 64> result;\n",
      "    const int loop_count = 256 / 64; // 256 elements, 64 elements per vector\n",
      "    ::aie::vector<int8_t, 64> offset_vector = ::aie::broadcast(offset); // Broadcast the offset\n",
      "\n",
      "    for(int j = 0; j < loop_count; j++) {\n",
      "        buffer = ::aie::load_v<64>(in_buffer); // Load 64 elements from input buffer\n",
      "        result = buffer + offset_vector; // Add offset to each element\n",
      "        ::aie::store_v(out_buffer, result); // Store the result back to output buffer\n",
      "        in_buffer += 64; // Move to the next segment of input buffer\n",
      "        out_buffer += 64; // Move to the next segment of output buffer\n",
      "    }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "coder.reset_history()\n",
    "response_with_rag = coder(prompt_with_context)\n",
    "print(f\"Code after {response_with_rag['attempt']} re-compilations:\")\n",
    "print(response_with_rag['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc50a0c0-3328-481c-b170-962a74f65c87",
   "metadata": {},
   "source": [
    "Immediately we can tell that the output tells a much more compelling story of optimization. It uses AIE namespaces, API functions like `::aie::broadcast` and buffer allocations using AIE vectors like `::aie::vector<int8_t, 64>`. So how does this kernel match up against a vanilla GPT-4o-mini solution?\n",
    "\n",
    "## Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1805d209-afa5-4004-9815-6fa4cb6f68f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Save vanilla gpt-4o-mini result to json\n",
    "solutions_path = Path(\"results/rag/gpt-4o-mini\")\n",
    "solutions_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "result = {\"code\": coder.extract_codeblock(response['response'])}\n",
    "solution_file = solutions_path / \"add_offset_int8.json\"\n",
    "with solution_file.open('w') as file:\n",
    "    json.dump(result, file, indent=4)\n",
    "\n",
    "# Save RAG-enhanced gpt-4o-mini result to json\n",
    "solutions_path_rag = Path(\"results/rag/gpt-4o-mini_rag\")\n",
    "solutions_path_rag.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "result_rag = {\"code\": coder.extract_codeblock(response_with_rag['response'])}\n",
    "solution_file = solutions_path_rag / \"add_offset_int8.json\"\n",
    "with solution_file.open('w') as file:\n",
    "    json.dump(result_rag, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a3ee43-89a1-4b69-b44c-87889b3677f5",
   "metadata": {},
   "source": [
    "We'll loop over only the 2 model solutions to get their performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9af2a7b5-e64d-4dea-bf92-52397903888b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating gpt-4o-mini\n",
      "\n",
      "Kernel: add_offset_int8_wrapper\n",
      "results/rag/evaluations/gpt-4o-mini/add_offset_int8_wrapper.mlir generated successfully\n",
      "add_offset_int8_wrapper.xclbin, add_offset_int8_wrapper.bin built\n",
      "Trace written to results/rag/evaluations/gpt-4o-mini/add_offset_int8_wrapper_trace.json\n",
      "Result: Pass\n",
      "Passed: 1/1\n",
      "Evaluating gpt-4o-mini_rag\n",
      "\n",
      "Kernel: add_offset_int8_wrapper\n",
      "results/rag/evaluations/gpt-4o-mini_rag/add_offset_int8_wrapper.mlir generated successfully\n",
      "add_offset_int8_wrapper.xclbin, add_offset_int8_wrapper.bin built\n",
      "Trace written to results/rag/evaluations/gpt-4o-mini_rag/add_offset_int8_wrapper_trace.json\n",
      "Result: Pass\n",
      "Passed: 1/1\n"
     ]
    }
   ],
   "source": [
    "from npueval import run_functional_tests\n",
    "\n",
    "for model in ['gpt-4o-mini', 'gpt-4o-mini_rag']:\n",
    "    print(f\"Evaluating {model}\")\n",
    "    run_functional_tests([sample], \n",
    "                     solutions=f\"results/rag/{model}\",\n",
    "                     results_path=f\"results/rag/evaluations/{model}\",\n",
    "                     overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85151b50-ece8-43d8-88fe-98f1318e64dd",
   "metadata": {},
   "source": [
    "Both solutions pass the functional tests, but the RAG solution significantly reduces the number of cycles for the same workload! That's an over 70x improvement in cycle count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b04c5a84-7e2b-4932-b304-79971bf844a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini\n",
      "-----------\n",
      "Result:       Pass\n",
      "Total cycles: 2502\n",
      "VPU cycles:   0\n",
      "Vector score: 0.0\n",
      "\n",
      "gpt-4o-mini_rag\n",
      "---------------\n",
      "Result:       Pass\n",
      "Total cycles: 35\n",
      "VPU cycles:   2\n",
      "Vector score: 5.714285714285714\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in ['gpt-4o-mini', 'gpt-4o-mini_rag']:\n",
    "    print(model)\n",
    "    print(len(model)*\"-\")\n",
    "    with open(f\"results/rag/evaluations/{model}/add_offset_int8_wrapper.json\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    print(f\"Result:       {data['result']}\")\n",
    "    print(f\"Total cycles: {data['total_cycles']}\")\n",
    "    print(f\"VPU cycles:   {data['vector_cycles']}\")\n",
    "    print(f\"Vector score: {data['vector_score']*100}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6331fdd-3208-42a5-ab62-3c163223a432",
   "metadata": {},
   "source": [
    "Note that even though the vectorized solution achieves a significant decrease in cycle count the vector score is still relatively low at 5% - this is because not everything can run on the VPU and inevitably some operations will require the scalar unit, hence achieving a perfect 100% is largely unachievable. It is a good metric to track however as a proxy for vectorization, since a score of 0 gives us a very clear flag that the VPU is being unused which is not what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5503f8a3-c59a-4d54-ac7d-56d731a14762",
   "metadata": {},
   "source": [
    "-----\n",
    "Copyright© 2025 AMD, Inc SPDX-License-Identifier: MIT "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironenv",
   "language": "python",
   "name": "ironenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
